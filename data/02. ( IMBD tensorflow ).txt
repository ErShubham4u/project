import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import imdb

num_words = 10000
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)
print(f"Training data: {len(train_data)}, Test data: {len(test_data)}")



from tensorflow.keras.preprocessing.sequence import pad_sequences
max_length = 250
train_data = pad_sequences(train_data, maxlen=max_length, padding='post')
test_data = pad_sequences(test_data, maxlen=max_length, padding='post')
print(f"Shape of training data: {train_data.shape}")






model = keras.Sequential([
    keras.layers.Embedding(input_dim=num_words, output_dim=32, input_length=max_length),
    keras.layers.Conv1D(32, 5, activation='relu'),
    keras.layers.MaxPooling1D(2),
    keras.layers.LSTM(32, return_sequences=True),
    keras.layers.LSTM(32),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation='sigmoid')
])
model.build(input_shape=(None, max_length)) 
model.summary()





model.compile(
    loss='binary_crossentropy',  
    optimizer='adam',           
    metrics=['accuracy']   
)
history = model.fit(train_data, train_labels, epochs=5, batch_size=64, validation_data=(test_data, test_labels))







 import matplotlib.pyplot as plt
 train_loss = history.history['loss']
 val_loss = history.history['val_loss']
 epochs = range(1, len(train_acc) + 1)
 plt.subplot(1, 2, 2)
 plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
 plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')
 plt.xlabel('Epochs')
 plt.ylabel('Loss')
 plt.title('Training vs Validation Loss')
 plt.legend()
 plt.show()





 test_loss, test_acc = model.evaluate(test_data, test_labels)
 print(f"Test Accuracy: {test_acc:.4f}")






 import numpy as np
 predictions = model.predict(test_data[:5])
 predictions = (predictions > 0.5).astype(int)
 print("Predictions:", predictions.flatten())
 print("Actual Labels:", test_labels[:5])











 import tensorflow as tf
 from tensorflow.keras.preprocessing.text import Tokenizer
 from tensorflow.keras.preprocessing.sequence import pad_sequences
 word_index = imdb.get_word_index()
 def predict_sentiment(review):
 words = review.lower().split()
 encoded_review = [word_index.get(word, 2) for word in words]
 padded_review = pad_sequences([encoded_review], maxlen=max_length, padding='post')
 prediction = model.predict(padded_review)[0][0]
 sentiment = "Positive " if prediction > 0.5 else "Negative "
 print(f"Review Sentiment: {sentiment} (Confidence: {prediction:.4f})")


 sample_review = "The movie was fantastic and had an amazing storyline"
 predict_sentiment(sample_review)
 sample_review2 = "The movie was good"
 predict_sentiment(sample_review2)













